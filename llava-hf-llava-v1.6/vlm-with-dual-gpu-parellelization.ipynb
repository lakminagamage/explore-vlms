{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nimport torch\nfrom accelerate import dispatch_model, infer_auto_device_map\nimport gc\n\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\n    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",  \n    max_memory={0: \"13GB\", 1: \"13GB\"} \n)\n\n# Alternative: Manual device mapping if auto doesn't work well\n# model = LlavaNextForConditionalGeneration.from_pretrained(\n#     \"llava-hf/llava-v1.6-mistral-7b-hf\",\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )\n# device_map = infer_auto_device_map(model, max_memory={0: \"13GB\", 1: \"13GB\"})\n# model = dispatch_model(model, device_map=device_map)\n\nprint(\"Model device map:\")\nfor name, module in model.named_modules():\n    if hasattr(module, 'weight') and hasattr(module.weight, 'device'):\n        print(f\"{name}: {module.weight.device}\")\n\nfrom PIL import Image\nimport requests\n\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n\ninputs = processor(prompt, image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    output = model.generate(\n        **inputs, \n        max_new_tokens=100,\n        do_sample=False,\n        pad_token_id=processor.tokenizer.eos_token_id\n    )\n\nresponse = processor.decode(output[0], skip_special_tokens=True)\nprint(response)\n\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i} memory: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB / {torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T06:42:15.852587Z","iopub.execute_input":"2025-08-25T06:42:15.852772Z","iopub.status.idle":"2025-08-25T06:45:45.246559Z","shell.execute_reply.started":"2025-08-25T06:42:15.852755Z","shell.execute_reply":"2025-08-25T06:45:45.245833Z"}},"outputs":[{"name":"stderr","text":"2025-08-25 06:42:32.190104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756104152.513277      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756104152.606106      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Available GPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce9db0a7f3de4204889058cc2cde5e03"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"362121ea821943d1b4aed5b16fac2abc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bdb03793e3401baca78697f0643b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c74b3ee49f24f11b1530ffe27b6d7bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20c1f8cc90043d48ed515c4bce6317a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6f5607af814f5f84b73c4489e24a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cab7db2d0030441ea966fa88e4c0e9b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694547618d584d29a90a064714404f65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba4ed2e89b014b8d892c4945fed0e327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d62c716a41c41bdba60f15297ef9c30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301bfcf641a64a31abebce9e0ae55bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060f2dc156da4f1486e4123e8e8ae2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b0364d13c9d4b5f975e974086231094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68a5ead50bd146f48ec80ec3cb5478bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923d1043996f4cfeb47b5dc5632be9e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039e6ff5910245bf98d79af91f078408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1254a64abc4843bf99fd4b0ef5545fa6"}},"metadata":{}},{"name":"stdout","text":"Model device map:\nmodel.vision_tower.vision_model.embeddings.patch_embedding: cuda:0\nmodel.vision_tower.vision_model.embeddings.position_embedding: cuda:0\nmodel.vision_tower.vision_model.pre_layrnorm: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.0.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.1.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.2.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.3.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.4.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.5.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.6.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.7.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.8.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.9.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.10.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.11.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.12.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.13.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.14.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.15.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.16.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.17.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.18.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.19.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.20.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.21.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.22.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.layer_norm1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.mlp.fc1: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.mlp.fc2: cuda:0\nmodel.vision_tower.vision_model.encoder.layers.23.layer_norm2: cuda:0\nmodel.vision_tower.vision_model.post_layernorm: cuda:0\nmodel.multi_modal_projector.linear_1: cuda:0\nmodel.multi_modal_projector.linear_2: cuda:0\nmodel.language_model.embed_tokens: cuda:0\nmodel.language_model.layers.0.self_attn.q_proj: cuda:0\nmodel.language_model.layers.0.self_attn.k_proj: cuda:0\nmodel.language_model.layers.0.self_attn.v_proj: cuda:0\nmodel.language_model.layers.0.self_attn.o_proj: cuda:0\nmodel.language_model.layers.0.mlp.gate_proj: cuda:0\nmodel.language_model.layers.0.mlp.up_proj: cuda:0\nmodel.language_model.layers.0.mlp.down_proj: cuda:0\nmodel.language_model.layers.0.input_layernorm: cuda:0\nmodel.language_model.layers.0.post_attention_layernorm: cuda:0\nmodel.language_model.layers.1.self_attn.q_proj: cuda:0\nmodel.language_model.layers.1.self_attn.k_proj: cuda:0\nmodel.language_model.layers.1.self_attn.v_proj: cuda:0\nmodel.language_model.layers.1.self_attn.o_proj: cuda:0\nmodel.language_model.layers.1.mlp.gate_proj: cuda:0\nmodel.language_model.layers.1.mlp.up_proj: cuda:0\nmodel.language_model.layers.1.mlp.down_proj: cuda:0\nmodel.language_model.layers.1.input_layernorm: cuda:0\nmodel.language_model.layers.1.post_attention_layernorm: cuda:0\nmodel.language_model.layers.2.self_attn.q_proj: cuda:0\nmodel.language_model.layers.2.self_attn.k_proj: cuda:0\nmodel.language_model.layers.2.self_attn.v_proj: cuda:0\nmodel.language_model.layers.2.self_attn.o_proj: cuda:0\nmodel.language_model.layers.2.mlp.gate_proj: cuda:0\nmodel.language_model.layers.2.mlp.up_proj: cuda:0\nmodel.language_model.layers.2.mlp.down_proj: cuda:0\nmodel.language_model.layers.2.input_layernorm: cuda:0\nmodel.language_model.layers.2.post_attention_layernorm: cuda:0\nmodel.language_model.layers.3.self_attn.q_proj: cuda:0\nmodel.language_model.layers.3.self_attn.k_proj: cuda:0\nmodel.language_model.layers.3.self_attn.v_proj: cuda:0\nmodel.language_model.layers.3.self_attn.o_proj: cuda:0\nmodel.language_model.layers.3.mlp.gate_proj: cuda:0\nmodel.language_model.layers.3.mlp.up_proj: cuda:0\nmodel.language_model.layers.3.mlp.down_proj: cuda:0\nmodel.language_model.layers.3.input_layernorm: cuda:0\nmodel.language_model.layers.3.post_attention_layernorm: cuda:0\nmodel.language_model.layers.4.self_attn.q_proj: cuda:0\nmodel.language_model.layers.4.self_attn.k_proj: cuda:0\nmodel.language_model.layers.4.self_attn.v_proj: cuda:0\nmodel.language_model.layers.4.self_attn.o_proj: cuda:0\nmodel.language_model.layers.4.mlp.gate_proj: cuda:0\nmodel.language_model.layers.4.mlp.up_proj: cuda:0\nmodel.language_model.layers.4.mlp.down_proj: cuda:0\nmodel.language_model.layers.4.input_layernorm: cuda:0\nmodel.language_model.layers.4.post_attention_layernorm: cuda:0\nmodel.language_model.layers.5.self_attn.q_proj: cuda:0\nmodel.language_model.layers.5.self_attn.k_proj: cuda:0\nmodel.language_model.layers.5.self_attn.v_proj: cuda:0\nmodel.language_model.layers.5.self_attn.o_proj: cuda:0\nmodel.language_model.layers.5.mlp.gate_proj: cuda:0\nmodel.language_model.layers.5.mlp.up_proj: cuda:0\nmodel.language_model.layers.5.mlp.down_proj: cuda:0\nmodel.language_model.layers.5.input_layernorm: cuda:0\nmodel.language_model.layers.5.post_attention_layernorm: cuda:0\nmodel.language_model.layers.6.self_attn.q_proj: cuda:0\nmodel.language_model.layers.6.self_attn.k_proj: cuda:0\nmodel.language_model.layers.6.self_attn.v_proj: cuda:0\nmodel.language_model.layers.6.self_attn.o_proj: cuda:0\nmodel.language_model.layers.6.mlp.gate_proj: cuda:0\nmodel.language_model.layers.6.mlp.up_proj: cuda:0\nmodel.language_model.layers.6.mlp.down_proj: cuda:0\nmodel.language_model.layers.6.input_layernorm: cuda:0\nmodel.language_model.layers.6.post_attention_layernorm: cuda:0\nmodel.language_model.layers.7.self_attn.q_proj: cuda:0\nmodel.language_model.layers.7.self_attn.k_proj: cuda:0\nmodel.language_model.layers.7.self_attn.v_proj: cuda:0\nmodel.language_model.layers.7.self_attn.o_proj: cuda:0\nmodel.language_model.layers.7.mlp.gate_proj: cuda:0\nmodel.language_model.layers.7.mlp.up_proj: cuda:0\nmodel.language_model.layers.7.mlp.down_proj: cuda:0\nmodel.language_model.layers.7.input_layernorm: cuda:0\nmodel.language_model.layers.7.post_attention_layernorm: cuda:0\nmodel.language_model.layers.8.self_attn.q_proj: cuda:0\nmodel.language_model.layers.8.self_attn.k_proj: cuda:0\nmodel.language_model.layers.8.self_attn.v_proj: cuda:0\nmodel.language_model.layers.8.self_attn.o_proj: cuda:0\nmodel.language_model.layers.8.mlp.gate_proj: cuda:0\nmodel.language_model.layers.8.mlp.up_proj: cuda:0\nmodel.language_model.layers.8.mlp.down_proj: cuda:0\nmodel.language_model.layers.8.input_layernorm: cuda:0\nmodel.language_model.layers.8.post_attention_layernorm: cuda:0\nmodel.language_model.layers.9.self_attn.q_proj: cuda:0\nmodel.language_model.layers.9.self_attn.k_proj: cuda:0\nmodel.language_model.layers.9.self_attn.v_proj: cuda:0\nmodel.language_model.layers.9.self_attn.o_proj: cuda:0\nmodel.language_model.layers.9.mlp.gate_proj: cuda:0\nmodel.language_model.layers.9.mlp.up_proj: cuda:0\nmodel.language_model.layers.9.mlp.down_proj: cuda:0\nmodel.language_model.layers.9.input_layernorm: cuda:0\nmodel.language_model.layers.9.post_attention_layernorm: cuda:0\nmodel.language_model.layers.10.self_attn.q_proj: cuda:0\nmodel.language_model.layers.10.self_attn.k_proj: cuda:0\nmodel.language_model.layers.10.self_attn.v_proj: cuda:0\nmodel.language_model.layers.10.self_attn.o_proj: cuda:0\nmodel.language_model.layers.10.mlp.gate_proj: cuda:0\nmodel.language_model.layers.10.mlp.up_proj: cuda:0\nmodel.language_model.layers.10.mlp.down_proj: cuda:0\nmodel.language_model.layers.10.input_layernorm: cuda:0\nmodel.language_model.layers.10.post_attention_layernorm: cuda:0\nmodel.language_model.layers.11.self_attn.q_proj: cuda:0\nmodel.language_model.layers.11.self_attn.k_proj: cuda:0\nmodel.language_model.layers.11.self_attn.v_proj: cuda:0\nmodel.language_model.layers.11.self_attn.o_proj: cuda:0\nmodel.language_model.layers.11.mlp.gate_proj: cuda:0\nmodel.language_model.layers.11.mlp.up_proj: cuda:0\nmodel.language_model.layers.11.mlp.down_proj: cuda:0\nmodel.language_model.layers.11.input_layernorm: cuda:0\nmodel.language_model.layers.11.post_attention_layernorm: cuda:0\nmodel.language_model.layers.12.self_attn.q_proj: cuda:0\nmodel.language_model.layers.12.self_attn.k_proj: cuda:0\nmodel.language_model.layers.12.self_attn.v_proj: cuda:0\nmodel.language_model.layers.12.self_attn.o_proj: cuda:0\nmodel.language_model.layers.12.mlp.gate_proj: cuda:0\nmodel.language_model.layers.12.mlp.up_proj: cuda:0\nmodel.language_model.layers.12.mlp.down_proj: cuda:0\nmodel.language_model.layers.12.input_layernorm: cuda:0\nmodel.language_model.layers.12.post_attention_layernorm: cuda:0\nmodel.language_model.layers.13.self_attn.q_proj: cuda:0\nmodel.language_model.layers.13.self_attn.k_proj: cuda:0\nmodel.language_model.layers.13.self_attn.v_proj: cuda:0\nmodel.language_model.layers.13.self_attn.o_proj: cuda:0\nmodel.language_model.layers.13.mlp.gate_proj: cuda:0\nmodel.language_model.layers.13.mlp.up_proj: cuda:0\nmodel.language_model.layers.13.mlp.down_proj: cuda:0\nmodel.language_model.layers.13.input_layernorm: cuda:0\nmodel.language_model.layers.13.post_attention_layernorm: cuda:0\nmodel.language_model.layers.14.self_attn.q_proj: cuda:0\nmodel.language_model.layers.14.self_attn.k_proj: cuda:0\nmodel.language_model.layers.14.self_attn.v_proj: cuda:0\nmodel.language_model.layers.14.self_attn.o_proj: cuda:0\nmodel.language_model.layers.14.mlp.gate_proj: cuda:0\nmodel.language_model.layers.14.mlp.up_proj: cuda:0\nmodel.language_model.layers.14.mlp.down_proj: cuda:0\nmodel.language_model.layers.14.input_layernorm: cuda:0\nmodel.language_model.layers.14.post_attention_layernorm: cuda:0\nmodel.language_model.layers.15.self_attn.q_proj: cuda:1\nmodel.language_model.layers.15.self_attn.k_proj: cuda:1\nmodel.language_model.layers.15.self_attn.v_proj: cuda:1\nmodel.language_model.layers.15.self_attn.o_proj: cuda:1\nmodel.language_model.layers.15.mlp.gate_proj: cuda:1\nmodel.language_model.layers.15.mlp.up_proj: cuda:1\nmodel.language_model.layers.15.mlp.down_proj: cuda:1\nmodel.language_model.layers.15.input_layernorm: cuda:1\nmodel.language_model.layers.15.post_attention_layernorm: cuda:1\nmodel.language_model.layers.16.self_attn.q_proj: cuda:1\nmodel.language_model.layers.16.self_attn.k_proj: cuda:1\nmodel.language_model.layers.16.self_attn.v_proj: cuda:1\nmodel.language_model.layers.16.self_attn.o_proj: cuda:1\nmodel.language_model.layers.16.mlp.gate_proj: cuda:1\nmodel.language_model.layers.16.mlp.up_proj: cuda:1\nmodel.language_model.layers.16.mlp.down_proj: cuda:1\nmodel.language_model.layers.16.input_layernorm: cuda:1\nmodel.language_model.layers.16.post_attention_layernorm: cuda:1\nmodel.language_model.layers.17.self_attn.q_proj: cuda:1\nmodel.language_model.layers.17.self_attn.k_proj: cuda:1\nmodel.language_model.layers.17.self_attn.v_proj: cuda:1\nmodel.language_model.layers.17.self_attn.o_proj: cuda:1\nmodel.language_model.layers.17.mlp.gate_proj: cuda:1\nmodel.language_model.layers.17.mlp.up_proj: cuda:1\nmodel.language_model.layers.17.mlp.down_proj: cuda:1\nmodel.language_model.layers.17.input_layernorm: cuda:1\nmodel.language_model.layers.17.post_attention_layernorm: cuda:1\nmodel.language_model.layers.18.self_attn.q_proj: cuda:1\nmodel.language_model.layers.18.self_attn.k_proj: cuda:1\nmodel.language_model.layers.18.self_attn.v_proj: cuda:1\nmodel.language_model.layers.18.self_attn.o_proj: cuda:1\nmodel.language_model.layers.18.mlp.gate_proj: cuda:1\nmodel.language_model.layers.18.mlp.up_proj: cuda:1\nmodel.language_model.layers.18.mlp.down_proj: cuda:1\nmodel.language_model.layers.18.input_layernorm: cuda:1\nmodel.language_model.layers.18.post_attention_layernorm: cuda:1\nmodel.language_model.layers.19.self_attn.q_proj: cuda:1\nmodel.language_model.layers.19.self_attn.k_proj: cuda:1\nmodel.language_model.layers.19.self_attn.v_proj: cuda:1\nmodel.language_model.layers.19.self_attn.o_proj: cuda:1\nmodel.language_model.layers.19.mlp.gate_proj: cuda:1\nmodel.language_model.layers.19.mlp.up_proj: cuda:1\nmodel.language_model.layers.19.mlp.down_proj: cuda:1\nmodel.language_model.layers.19.input_layernorm: cuda:1\nmodel.language_model.layers.19.post_attention_layernorm: cuda:1\nmodel.language_model.layers.20.self_attn.q_proj: cuda:1\nmodel.language_model.layers.20.self_attn.k_proj: cuda:1\nmodel.language_model.layers.20.self_attn.v_proj: cuda:1\nmodel.language_model.layers.20.self_attn.o_proj: cuda:1\nmodel.language_model.layers.20.mlp.gate_proj: cuda:1\nmodel.language_model.layers.20.mlp.up_proj: cuda:1\nmodel.language_model.layers.20.mlp.down_proj: cuda:1\nmodel.language_model.layers.20.input_layernorm: cuda:1\nmodel.language_model.layers.20.post_attention_layernorm: cuda:1\nmodel.language_model.layers.21.self_attn.q_proj: cuda:1\nmodel.language_model.layers.21.self_attn.k_proj: cuda:1\nmodel.language_model.layers.21.self_attn.v_proj: cuda:1\nmodel.language_model.layers.21.self_attn.o_proj: cuda:1\nmodel.language_model.layers.21.mlp.gate_proj: cuda:1\nmodel.language_model.layers.21.mlp.up_proj: cuda:1\nmodel.language_model.layers.21.mlp.down_proj: cuda:1\nmodel.language_model.layers.21.input_layernorm: cuda:1\nmodel.language_model.layers.21.post_attention_layernorm: cuda:1\nmodel.language_model.layers.22.self_attn.q_proj: cuda:1\nmodel.language_model.layers.22.self_attn.k_proj: cuda:1\nmodel.language_model.layers.22.self_attn.v_proj: cuda:1\nmodel.language_model.layers.22.self_attn.o_proj: cuda:1\nmodel.language_model.layers.22.mlp.gate_proj: cuda:1\nmodel.language_model.layers.22.mlp.up_proj: cuda:1\nmodel.language_model.layers.22.mlp.down_proj: cuda:1\nmodel.language_model.layers.22.input_layernorm: cuda:1\nmodel.language_model.layers.22.post_attention_layernorm: cuda:1\nmodel.language_model.layers.23.self_attn.q_proj: cuda:1\nmodel.language_model.layers.23.self_attn.k_proj: cuda:1\nmodel.language_model.layers.23.self_attn.v_proj: cuda:1\nmodel.language_model.layers.23.self_attn.o_proj: cuda:1\nmodel.language_model.layers.23.mlp.gate_proj: cuda:1\nmodel.language_model.layers.23.mlp.up_proj: cuda:1\nmodel.language_model.layers.23.mlp.down_proj: cuda:1\nmodel.language_model.layers.23.input_layernorm: cuda:1\nmodel.language_model.layers.23.post_attention_layernorm: cuda:1\nmodel.language_model.layers.24.self_attn.q_proj: cuda:1\nmodel.language_model.layers.24.self_attn.k_proj: cuda:1\nmodel.language_model.layers.24.self_attn.v_proj: cuda:1\nmodel.language_model.layers.24.self_attn.o_proj: cuda:1\nmodel.language_model.layers.24.mlp.gate_proj: cuda:1\nmodel.language_model.layers.24.mlp.up_proj: cuda:1\nmodel.language_model.layers.24.mlp.down_proj: cuda:1\nmodel.language_model.layers.24.input_layernorm: cuda:1\nmodel.language_model.layers.24.post_attention_layernorm: cuda:1\nmodel.language_model.layers.25.self_attn.q_proj: cuda:1\nmodel.language_model.layers.25.self_attn.k_proj: cuda:1\nmodel.language_model.layers.25.self_attn.v_proj: cuda:1\nmodel.language_model.layers.25.self_attn.o_proj: cuda:1\nmodel.language_model.layers.25.mlp.gate_proj: cuda:1\nmodel.language_model.layers.25.mlp.up_proj: cuda:1\nmodel.language_model.layers.25.mlp.down_proj: cuda:1\nmodel.language_model.layers.25.input_layernorm: cuda:1\nmodel.language_model.layers.25.post_attention_layernorm: cuda:1\nmodel.language_model.layers.26.self_attn.q_proj: cuda:1\nmodel.language_model.layers.26.self_attn.k_proj: cuda:1\nmodel.language_model.layers.26.self_attn.v_proj: cuda:1\nmodel.language_model.layers.26.self_attn.o_proj: cuda:1\nmodel.language_model.layers.26.mlp.gate_proj: cuda:1\nmodel.language_model.layers.26.mlp.up_proj: cuda:1\nmodel.language_model.layers.26.mlp.down_proj: cuda:1\nmodel.language_model.layers.26.input_layernorm: cuda:1\nmodel.language_model.layers.26.post_attention_layernorm: cuda:1\nmodel.language_model.layers.27.self_attn.q_proj: cuda:1\nmodel.language_model.layers.27.self_attn.k_proj: cuda:1\nmodel.language_model.layers.27.self_attn.v_proj: cuda:1\nmodel.language_model.layers.27.self_attn.o_proj: cuda:1\nmodel.language_model.layers.27.mlp.gate_proj: cuda:1\nmodel.language_model.layers.27.mlp.up_proj: cuda:1\nmodel.language_model.layers.27.mlp.down_proj: cuda:1\nmodel.language_model.layers.27.input_layernorm: cuda:1\nmodel.language_model.layers.27.post_attention_layernorm: cuda:1\nmodel.language_model.layers.28.self_attn.q_proj: cuda:1\nmodel.language_model.layers.28.self_attn.k_proj: cuda:1\nmodel.language_model.layers.28.self_attn.v_proj: cuda:1\nmodel.language_model.layers.28.self_attn.o_proj: cuda:1\nmodel.language_model.layers.28.mlp.gate_proj: cuda:1\nmodel.language_model.layers.28.mlp.up_proj: cuda:1\nmodel.language_model.layers.28.mlp.down_proj: cuda:1\nmodel.language_model.layers.28.input_layernorm: cuda:1\nmodel.language_model.layers.28.post_attention_layernorm: cuda:1\nmodel.language_model.layers.29.self_attn.q_proj: cuda:1\nmodel.language_model.layers.29.self_attn.k_proj: cuda:1\nmodel.language_model.layers.29.self_attn.v_proj: cuda:1\nmodel.language_model.layers.29.self_attn.o_proj: cuda:1\nmodel.language_model.layers.29.mlp.gate_proj: cuda:1\nmodel.language_model.layers.29.mlp.up_proj: cuda:1\nmodel.language_model.layers.29.mlp.down_proj: cuda:1\nmodel.language_model.layers.29.input_layernorm: cuda:1\nmodel.language_model.layers.29.post_attention_layernorm: cuda:1\nmodel.language_model.layers.30.self_attn.q_proj: cuda:1\nmodel.language_model.layers.30.self_attn.k_proj: cuda:1\nmodel.language_model.layers.30.self_attn.v_proj: cuda:1\nmodel.language_model.layers.30.self_attn.o_proj: cuda:1\nmodel.language_model.layers.30.mlp.gate_proj: cuda:1\nmodel.language_model.layers.30.mlp.up_proj: cuda:1\nmodel.language_model.layers.30.mlp.down_proj: cuda:1\nmodel.language_model.layers.30.input_layernorm: cuda:1\nmodel.language_model.layers.30.post_attention_layernorm: cuda:1\nmodel.language_model.layers.31.self_attn.q_proj: cuda:1\nmodel.language_model.layers.31.self_attn.k_proj: cuda:1\nmodel.language_model.layers.31.self_attn.v_proj: cuda:1\nmodel.language_model.layers.31.self_attn.o_proj: cuda:1\nmodel.language_model.layers.31.mlp.gate_proj: cuda:1\nmodel.language_model.layers.31.mlp.up_proj: cuda:1\nmodel.language_model.layers.31.mlp.down_proj: cuda:1\nmodel.language_model.layers.31.input_layernorm: cuda:1\nmodel.language_model.layers.31.post_attention_layernorm: cuda:1\nmodel.language_model.norm: cuda:1\nlm_head: cuda:1\n","output_type":"stream"},{"name":"stderr","text":"You may have used the wrong order for inputs. `images` should be passed before `text`. The `images` and `text` inputs will be swapped. This behavior will be deprecated in transformers v4.47.\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[INST]  \nWhat is shown in this image? [/INST] The image appears to be a radar chart, which is a type of multivariate chart that displays values for multiple variables represented on axes starting from the same point. This particular radar chart is showing the performance of different models or systems across various metrics.\n\nThe axes represent different metrics or benchmarks, such as MM-Vet, MM-Vet, MM-Vet, MM-Vet, MM-Vet, MM-V\nGPU 0 memory: 6.95 GB / 7.43 GB\nGPU 1 memory: 7.16 GB / 7.61 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"url = \"https://static.toiimg.com/thumb/msid-113367147,width-1280,height-720,resizemode-72/113367147.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n\ninputs = processor(prompt, image, return_tensors=\"pt\")\n\nwith torch.no_grad(): \n    output = model.generate(\n        **inputs, \n        max_new_tokens=100,\n        do_sample=False,\n        pad_token_id=processor.tokenizer.eos_token_id\n    )\n\nresponse = processor.decode(output[0], skip_special_tokens=True)\nprint(response)\n\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i} memory: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB / {torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T06:49:42.039020Z","iopub.execute_input":"2025-08-25T06:49:42.039610Z","iopub.status.idle":"2025-08-25T06:49:52.427384Z","shell.execute_reply.started":"2025-08-25T06:49:42.039584Z","shell.execute_reply":"2025-08-25T06:49:52.426630Z"}},"outputs":[{"name":"stdout","text":"[INST]  \nWhat is shown in this image? [/INST] The image depicts an illustration of a classroom scene. A teacher is standing in front of a group of students, holding a stick or a rod, which is often used as a teaching tool in some educational settings. The students appear to be listening attentively to the teacher. The teacher's expression suggests they might be explaining something or giving instructions. The classroom setting is typical, with desks and chairs visible, and the students are dressed in casual attire. The image is likely meant\nGPU 0 memory: 6.95 GB / 7.43 GB\nGPU 1 memory: 7.16 GB / 7.61 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}